{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row added successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'latency': 10.0, 'area': 100.0, 'power': 50.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_row(data, alldata, status):\n",
    "    # 将status字典转换为DataFrame中行的形式\n",
    "    status_series = pd.Series(status)\n",
    "    \n",
    "    # 检查data中每一行的前8列\n",
    "    for index, row in data.iterrows():\n",
    "        if row[:8].equals(status_series):\n",
    "            return row  # 找到匹配的行，返回这一行\n",
    "    \n",
    "    # 如果在data中没有找到，检查alldata\n",
    "    for index, row in alldata.iterrows():\n",
    "        if row[:8].equals(status_series):\n",
    "            return row  # 找到匹配的行，返回这一行\n",
    "    \n",
    "    # 如果两个DataFrame都没有找到匹配的行，返回None\n",
    "    return None\n",
    "\n",
    "# 假设你已经有两个DataFrame data和alldata\n",
    "data = pd.DataFrame({\n",
    "    'core': [1, 2, 3],\n",
    "    'l1i_size': [32, 64, 128],\n",
    "    'l1d_size': [32, 64, 128],\n",
    "    'l2_size': [256, 512, 1024],\n",
    "    'l1d_assoc': [4, 4, 4],\n",
    "    'l1i_assoc': [4, 4, 4],\n",
    "    'l2_assoc': [8, 8, 8],\n",
    "    'sys_clock': [2.4, 2.5, 2.6],\n",
    "    'latency': [10, 20, 30],\n",
    "    'area': [100, 200, 300],\n",
    "    'power': [50, 60, 70]\n",
    "})\n",
    "\n",
    "alldata = pd.DataFrame({\n",
    "    'core': [1, 5, 6],\n",
    "    'l1i_size': [32, 512, 1024],\n",
    "    'l1d_size': [32, 512, 1024],\n",
    "    'l2_size': [256, 2048, 4096],\n",
    "    'l1d_assoc': [4, 4, 4],\n",
    "    'l1i_assoc': [4, 4, 4],\n",
    "    'l2_assoc': [8, 8, 8],\n",
    "    'sys_clock': [2.4, 2.8, 2.9],\n",
    "    'latency': [10, 50, 60],\n",
    "    'area': [100, 500, 600],\n",
    "    'power': [50, 90, 100]\n",
    "})\n",
    "\n",
    "# status结构体，现假定为一个字典，与DataFrame的列结构相同\n",
    "status = {\n",
    "    'core': 1,\n",
    "    'l1i_size': 32,\n",
    "    'l1d_size': 32,\n",
    "    'l2_size': 256,\n",
    "    'l1d_assoc': 4,\n",
    "    'l1i_assoc': 4,\n",
    "    'l2_assoc': 8,\n",
    "    'sys_clock': 2.4\n",
    "}\n",
    "\n",
    "result = find_row(data, alldata, status)\n",
    "row = find_row(data,alldata,status)\n",
    "# 检查结果是否非空，然后添加到data中\n",
    "if row is not None:\n",
    "    data = pd.concat([data, pd.DataFrame([row])], ignore_index=True)\n",
    "    print(\"Row added successfully!\")\n",
    "else:\n",
    "    print(\"No matching row found\")\n",
    "    \n",
    "# data\n",
    "\n",
    "result_dict = row[-3:].to_dict()\n",
    "\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T07:06:19.649126Z",
     "start_time": "2024-12-05T07:06:19.455521Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"float\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m     value \u001b[38;5;241m=\u001b[39m value_net(state_tensor)\n\u001b[1;32m    146\u001b[0m     next_values\u001b[38;5;241m.\u001b[39mappend(value\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 148\u001b[0m advantages, returns \u001b[38;5;241m=\u001b[39m compute_advantages(buffer\u001b[38;5;241m.\u001b[39mrewards, buffer\u001b[38;5;241m.\u001b[39mdones, buffer\u001b[38;5;241m.\u001b[39mvalues, next_values[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# 转换为张量\u001b[39;00m\n\u001b[1;32m    151\u001b[0m states \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m episode_states \u001b[38;5;129;01min\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mstates \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m episode_states]\n",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m, in \u001b[0;36mcompute_advantages\u001b[0;34m(rewards, dones, values, next_value)\u001b[0m\n\u001b[1;32m    103\u001b[0m gae \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rewards))):\n\u001b[0;32m--> 105\u001b[0m     delta \u001b[38;5;241m=\u001b[39m rewards[step] \u001b[38;5;241m+\u001b[39m GAMMA \u001b[38;5;241m*\u001b[39m next_value \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones[step]) \u001b[38;5;241m-\u001b[39m values[step]\n\u001b[1;32m    106\u001b[0m     gae \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m+\u001b[39m GAMMA \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.95\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones[step]) \u001b[38;5;241m*\u001b[39m gae\n\u001b[1;32m    107\u001b[0m     advantages\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, gae)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"float\") to list"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# 超参数\n",
    "NUM_AGENTS = 2\n",
    "STATE_DIM = 24\n",
    "ACTION_DIM = 2\n",
    "HIDDEN_SIZE = 128\n",
    "LR = 3e-4\n",
    "GAMMA = 0.99\n",
    "CLIP_EPS = 0.2\n",
    "PPO_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# 简单的多智能体环境\n",
    "class SimpleMultiAgentEnv:\n",
    "    def reset(self):\n",
    "        # 返回每个智能体的初始状态\n",
    "        return [np.random.randn(STATE_DIM) for _ in range(NUM_AGENTS)]\n",
    "    \n",
    "    def step(self, actions):\n",
    "        # 返回下一个状态、奖励、完成标志和额外信息\n",
    "        next_states = [np.random.randn(STATE_DIM) for _ in range(NUM_AGENTS)]\n",
    "        rewards = [1.0 for _ in range(NUM_AGENTS)]  # 简单的固定奖励\n",
    "        done = False\n",
    "        return next_states, rewards, done, {}\n",
    "\n",
    "# 策略网络\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.action_head = nn.Linear(hidden_size, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        action_logits = self.action_head(x)\n",
    "        return action_logits\n",
    "\n",
    "# 价值网络\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        value = self.fc(x)\n",
    "        return value\n",
    "\n",
    "# 经验回放缓冲区\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "# 初始化环境和网络\n",
    "env = SimpleMultiAgentEnv()\n",
    "policies = [PolicyNetwork(STATE_DIM, ACTION_DIM, HIDDEN_SIZE) for _ in range(NUM_AGENTS)]\n",
    "value_net = ValueNetwork(STATE_DIM, HIDDEN_SIZE)\n",
    "optimizer_policy = optim.Adam([param for policy in policies for param in policy.parameters()], lr=LR)\n",
    "optimizer_value = optim.Adam(value_net.parameters(), lr=LR)\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "\n",
    "# 选择动作\n",
    "def select_action(state, policy):\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    logits = policy(state)\n",
    "    dist = Categorical(logits=logits.softmax(dim=-1))\n",
    "    action = dist.sample()\n",
    "    return action.item(), dist.log_prob(action), dist.entropy()\n",
    "\n",
    "# 计算优势\n",
    "def compute_advantages(rewards, dones, values, next_value):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + GAMMA * next_value * (1 - dones[step]) - values[step]\n",
    "        gae = delta + GAMMA * 0.95 * (1 - dones[step]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "        next_value = values[step]\n",
    "    returns = [adv + val for adv, val in zip(advantages, values)]\n",
    "    return advantages, returns\n",
    "\n",
    "# 主训练循环\n",
    "for episode in range(1000):\n",
    "    states = env.reset()\n",
    "    buffer.clear()\n",
    "    for step in range(MAX_STEPS):\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        for i, state in enumerate(states):\n",
    "            action, log_prob, _ = select_action(state, policies[i])\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            value = value_net(state_tensor)\n",
    "            values.append(value.item())\n",
    "        \n",
    "        buffer.states.append(states)\n",
    "        buffer.actions.append(actions)\n",
    "        buffer.log_probs.append(log_probs)\n",
    "        buffer.values.append(values)\n",
    "        \n",
    "        next_states, rewards, done, _ = env.step(actions)\n",
    "        buffer.rewards.append(rewards)\n",
    "        buffer.dones.append(done)\n",
    "        states = next_states\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # 计算最后一个状态的价值\n",
    "    next_values = []\n",
    "    for i, state in enumerate(states):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        value = value_net(state_tensor)\n",
    "        next_values.append(value.item())\n",
    "    \n",
    "    advantages, returns = compute_advantages(buffer.rewards, buffer.dones, buffer.values, next_values[0])\n",
    "    \n",
    "    # 转换为张量\n",
    "    states = [s for episode_states in buffer.states for s in episode_states]\n",
    "    actions = [a for episode_actions in buffer.actions for a in episode_actions]\n",
    "    old_log_probs = [lp for episode_lps in buffer.log_probs for lp in episode_lps]\n",
    "    advantages = torch.FloatTensor(advantages)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    \n",
    "    # 更新策略网络\n",
    "    for _ in range(PPO_EPOCHS):\n",
    "        # 随机采样批次\n",
    "        indices = np.random.randint(0, len(states), size=BATCH_SIZE)\n",
    "        sampled_states = torch.FloatTensor([states[i] for i in indices])\n",
    "        sampled_actions = torch.LongTensor([actions[i] for i in indices])\n",
    "        sampled_old_log_probs = torch.stack([old_log_probs[i] for i in indices])\n",
    "        sampled_advantages = advantages[indices]\n",
    "        sampled_returns = returns[indices]\n",
    "        \n",
    "        # 计算新的log_probs\n",
    "        new_log_probs = []\n",
    "        for i, policy in enumerate(policies):\n",
    "            logits = policy(sampled_states)\n",
    "            dist = Categorical(logits=logits.softmax(dim=-1))\n",
    "            log_prob = dist.log_prob(sampled_actions[i])\n",
    "            new_log_probs.append(log_prob)\n",
    "        \n",
    "        # 计算策略损失\n",
    "        ratios = torch.exp(torch.stack(new_log_probs) - sampled_old_log_probs)\n",
    "        surr1 = ratios * sampled_advantages\n",
    "        surr2 = torch.clamp(ratios, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * sampled_advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        # 计算价值损失\n",
    "        values = value_net(sampled_states).squeeze()\n",
    "        value_loss = nn.MSELoss()(values, sampled_returns)\n",
    "        \n",
    "        # 总损失\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer_policy.zero_grad()\n",
    "        optimizer_value.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_policy.step()\n",
    "        optimizer_value.step()\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"训练完成！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
